<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Trajectory Planning.">
  <meta name="keywords" content="ColaVLA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ColaVLA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://pqh22.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <!-- More Research Dropdown commented out -->
      <!--
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More Research</a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">HyperNeRF</a>
          <a class="navbar-item" href="https://nerfies.github.io">Nerfies</a>
          <a class="navbar-item" href="https://latentfusion.github.io">LatentFusion</a>
          <a class="navbar-item" href="https://photoshape.github.io">PhotoShape</a>
        </div>
      </div>
      -->
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel <br> Trajectory Planning in Autonomous Driving</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://pqh22.github.io">Qihang Peng</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_XZonAsAAAAJ&hl=en">Xuesong Chen</a><sup>2,3</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/pfxnb">Chenye Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://shishaoshuai.com/">Shaoshuai Shi</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University </span>
            <span class="author-block"><sup>2</sup>CUHK MMLab </span>
            <span class="author-block"><sup>3</sup>Voyager Research, Didi Chuxing </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.22939"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Paper Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.22939"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/pqh22/ColaVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision‚Äìlanguage models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision‚Äìlanguage‚Äìaction framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->




    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">
            <span class="icon">
              üî•
            </span>
            Highlights
          </h2>
          <div class="content has-text-justified">
            <ul>
              <!-- <li><strong>Unified Framework for Point-language Understanding and Generation.</strong> -->
              <li><strong>Cognitive Latent Reasoning (text CoT ‚Üí latent reasoning).</strong>
                We relocate chain-of-thought from discrete text to a compact latent space: an ego-adaptive router keeps only safety-critical cues, and a latent ‚Äúrethink & decide‚Äù stage produces a driving strategy without autoregressive text decoding.              </li>
              <li><strong>Hierarchical Parallel Planner (one-pass, causal, multi-scale).</strong>
                A multi-stage intent-to-motion decoder generates coarse-to-fine trajectories in parallel with a causality-preserving hybrid attention mask, enabling efficient and consistent multi-scale planning in a single forward pass.
              </li>
              <li><strong>Proxy Attention.</strong>
                We introduce a generalized proxy attention mechanism, allowing the selection of different proxy tokens based on task requirements, achieving linear computational complexity.
              </li>
              <li><strong>State-of-the-art performance on nuScenes</strong>
                Open-loop: <strong>0.30m</strong> average L2 and <strong>0.23%</strong> collision rate. Closed-loop (NeuroNCAP): <strong>3.48</strong> score with <strong>36.8%</strong> collision rate (top-1 strategy for realistic decision making).
              </li>
              <li>
                <strong>Low latency for real-time deployment.</strong>
                End-to-end inference latency is <strong>727 ms/frame</strong> on a single NVIDIA H20 (w/o flash-attention in the VLM), substantially faster than text-based VLM planners that require multiple forward passes.
              </li>
            </ul>
          </div>
          <div class="container is-max-desktop has-text-centered">
            <img src="./static/images/illustrate.png" 
                  alt="Illustration of reasoning paradigm" 
                  class="illustration-image">
            <p class="image-caption">From text chain-of-thought to cognitive latent reasoning for efficient planning.</p>
          </div>
        </div>
      </div>
    </div>



    <!-- Methods. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>

        <div class="content has-text-justified">
          <p>
            ColaVLA is a unified vision‚Äìlanguage‚Äìaction framework consisting of two core components:
            <strong>(1) Cognitive Latent Reasoner</strong> and <strong>(2) Hierarchical Parallel Planner</strong>.
            Multi-view image sequences are encoded into visual tokens (objects & maps), which are fused with an ego token and a fixed driving prompt.
            An <strong>ego-adaptive router</strong> selects safety-critical visual cues to form a compact pruned context.
            The model then performs a <strong>latent rethinking</strong> step with a bank of learnable <strong>meta-queries</strong>
            and finally outputs a discrete <strong>driving strategy</strong>.
          </p>
          <p>
            Conditioned on the selected strategy, we retrieve the corresponding <strong>meta-action</strong> from an action bank and expand it into
            <strong>multi-scale trajectory targets</strong> via temporal embeddings and resampling.
            The <strong>Hierarchical Parallel Planner</strong> concatenates the pruned context and all scale-wise targets, and performs
            <strong>one-pass parallel decoding</strong> with a <strong>causality-preserving hybrid mask</strong>:
            global context aggregation to all scales, bidirectional interaction within each scale, and strictly causal information flow from coarser to finer scales.
            This yields efficient, causal and interpretable multi-scale trajectories.
          </p>
        </div>
        <div class="container is-max-desktop has-text-centered">
          <img src="./static/images/framework.png" 
                alt="ColaVLA Framework" 
                class="illustration-image">
          <p class="image-caption">Overall framework of ColaVLA.</p>
        </div>
      </div>
    </div>
    <!--/ Method. -->

    <!-- Visualization. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualization</h2>
        <div class="content has-text-justified">
          <p>
            We visualize representative driving scenes and the predicted trajectories to highlight how ColaVLA couples compact latent reasoning
            with hierarchical parallel decoding. The qualitative results demonstrate robust behavior under complex multi-agent interactions,
            long-horizon intent understanding, and safety-critical scenarios.
          </p>
        </div>
        <div class="content">
          <img src="./static/images/visualization.png" alt="Visualization Image">
          <p class="image-caption">Qualitative visualization of planning results.</p>
        </div>
      </div>
    </div>
    <!--/ Visualization. -->


    <div class="container is-max-desktop">
      <!-- Experiments -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experiments</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Open-loop (nuScenes).</strong>
              ColaVLA achieves strong accuracy and safety with <strong>0.30m</strong> average L2 error and <strong>0.23%</strong> collision rate,
              demonstrating precise trajectory prediction while operating efficiently in the latent action space.
            </p>
            <p>
              <strong>Closed-loop (NeuroNCAP).</strong>
              ColaVLA establishes robust closed-loop driving with a <strong>3.48</strong> NeuroNCAP score and <strong>36.8%</strong> collision rate
              under a realistic top-1 strategy setting, showing improved safety and generalization in safety-critical scenarios.
            </p>
            <p>
              For detailed metrics, ablations, and additional analyses, please refer to the paper.
            </p>
          </div>
        </div>
      </div>
  
      <!-- <div class="columns is-centered">
        <div class="column is-half has-text-centered">
          <img src="./static/images/open_loop_results.png"
               alt="Open-loop Results"
               class="experiment-image">
          <p class="image-caption">Open-loop results on nuScenes.</p>
        </div>
        <div class="column is-half has-text-centered">
          <img src="./static/images/close_loop_results.png"
               alt="Closed-loop Results"
               class="experiment-image">
          <p class="image-caption">Closed-loop results on NeuroNCAP.</p>
        </div>
      </div> -->
      <!-- Open-loop Results -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <img src="./static/images/open_loop_results.png"
               alt="Open-loop Results"
               class="experiment-image">
          <p class="image-caption">Open-loop results on nuScenes.</p>
        </div>
      </div>
      
      <!-- Closed-loop Results -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <img src="./static/images/close_loop_results.png"
               alt="Closed-loop Results"
               class="experiment-image">
          <p class="image-caption">Closed-loop results on NeuroNCAP.</p>
        </div>
      </div>
    </div>
    <!-- / Experiments -->

    

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- BibTeX. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>  <!-- Âè™ËÆ©Ê†áÈ¢òÂ±Ö‰∏≠ -->
        <div class="bibtex-container">
          <pre>
            @misc{peng2025colavlaleveragingcognitivelatent,
              title={ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving}, 
              author={Qihang Peng and Xuesong Chen and Chenye Yang and Shaoshuai Shi and Hongsheng Li},
              year={2025},
              eprint={2512.22939},
              archivePrefix={arXiv},
              primaryClass={cs.CV},
              url={https://arxiv.org/abs/2512.22939}, 
            }
          </pre>
        </div>
      </div>
    </div>
    <!--/ BibTeX. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2512.22939">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/pqh22" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We thank  <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for their project page template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
