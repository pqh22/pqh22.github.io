

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='https://pqh22.github.io/images/pt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ProxyTransformation : Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding](https://arxiv.org/abs/2502.19247) \\
**Qihang Peng**, Henry Zheng, Gao Huang

[**Project**](https://speechresearch.github.io/fastspeech/) | [**Code**](https://github.com/pqh22/ProxyTransformation)
- Make full use of multimodal information in ego-centric 3D visual grounding for point enhancement.
- State-of-the-art on [EmbodiedScan](https://tai-wang.github.io/embodiedscan/) benchmark.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='https://pqh22.github.io/images/dg.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding](https://arxiv.org/abs/2502.19247) \\
Henry Zheng*, Shi Hao*, **Qihang Peng**, et al. 

[**ICLR 2025**](https://iclr.cc/virtual/2025/poster/28704) | [**AGC 2024**](https://opendrivelab.com/challenge2024/#multiview_3d_visual_grounding)
- Use LLM and Ground Truth to enhance semantic details in prompt to reduce the ambiguity during training.
- Extract individual view semantics and enriches visual representation with global scene-level semantic.
</div>
</div>