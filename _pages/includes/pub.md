<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2026 Main Conference</div><img src='https://pqh22.github.io/projects/ColaVLA/static/images/framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939) \\
**Qihang Peng**,â€ƒXuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li 

[**Project**](https://pqh22.github.io/projects/ColaVLA/index.html) | [**Code**](https://github.com/pqh22/ColaVLA)
- ColaVLA moves VLM reasoning into a compact latent space and decodes multi-scale causal trajectories in one pass.
- State-of-the-art in both open-loop and closed-loop settings with favorable efficiency and robustnesson on the nuScenes benchmark.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='https://pqh22.github.io/images/pt.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ProxyTransformation : Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding](https://arxiv.org/abs/2502.19247) \\
**Qihang Peng**, Henry Zheng, Gao Huang

[**Project**](https://pqh22.github.io/projects/ProxyTransformation/index.html) | [**Code**](https://github.com/pqh22/ProxyTransformation)
- Make full use of multimodal information in ego-centric 3D visual grounding for point enhancement.
- State-of-the-art on [EmbodiedScan](https://tai-wang.github.io/embodiedscan/) benchmark.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='https://pqh22.github.io/images/dg.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding](https://arxiv.org/abs/2502.19247) \\
Henry Zheng\*, Shi Hao\*, **Qihang Peng**, et al. 

[**ICLR 2025**](https://iclr.cc/virtual/2025/poster/28704) | [**AGC 2024**](https://opendrivelab.com/challenge2024/#multiview_3d_visual_grounding)
- Use LLM and Ground Truth to enhance semantic details in prompt to reduce the ambiguity during training.
- Extract individual view semantics and enriches visual representation with global scene-level semantic.
</div>
</div>
